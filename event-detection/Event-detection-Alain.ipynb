{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.cluster.hierarchy import ward, dendrogram,fcluster\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_event_split = ['id','userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse_dates = ['createdAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../twitter-swisscom/twex_event_corrected.tsv', sep=\"\\t\", encoding='utf-8', escapechar='\\\\', names=col_event_split, parse_dates=parse_dates, na_values='N', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514097914</td>\n",
       "      <td>17341045</td>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>Guuuuten Morgen! :-)</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7198282</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9516574359</td>\n",
       "      <td>14657884</td>\n",
       "      <td>2010-02-23 07:34:25</td>\n",
       "      <td>It has been a week or so.. and today I just co...</td>\n",
       "      <td>6.13396</td>\n",
       "      <td>46.1951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9516952605</td>\n",
       "      <td>14703863</td>\n",
       "      <td>2010-02-23 07:51:47</td>\n",
       "      <td>Getting ready..  http://twitpic.com/14v8gz</td>\n",
       "      <td>8.81749</td>\n",
       "      <td>47.2288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9517198943</td>\n",
       "      <td>14393717</td>\n",
       "      <td>2010-02-23 08:02:57</td>\n",
       "      <td>Un peu de réconfort liquide en take away après...</td>\n",
       "      <td>6.63254</td>\n",
       "      <td>46.5199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    userId           createdAt  \\\n",
       "0  9514097914  17341045 2010-02-23 05:55:51   \n",
       "1  9514846412   7198282 2010-02-23 06:22:40   \n",
       "2  9516574359  14657884 2010-02-23 07:34:25   \n",
       "3  9516952605  14703863 2010-02-23 07:51:47   \n",
       "4  9517198943  14393717 2010-02-23 08:02:57   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0                               Guuuuten Morgen! :-)    7.43926   46.9489   \n",
       "1  Still the best coffee in town — at La Stanza h...    8.53781   47.3678   \n",
       "2  It has been a week or so.. and today I just co...    6.13396   46.1951   \n",
       "3         Getting ready..  http://twitpic.com/14v8gz    8.81749   47.2288   \n",
       "4  Un peu de réconfort liquide en take away après...    6.63254   46.5199   \n",
       "\n",
       "  placeId  inReplyTo  placeLatitude  placeLongitude  \n",
       "0     NaN        NaN            NaN             NaN  \n",
       "1     NaN        NaN            NaN             NaN  \n",
       "2     NaN        NaN            NaN             NaN  \n",
       "3     NaN        NaN            NaN             NaN  \n",
       "4     NaN        NaN            NaN             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.drop(['placeId', 'inReplyTo'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different informations on the longitude and latitude are given, the columns longitude/latitude are the position of the Tweet as reported by the user or client application. The place longitude/latitude is  indicates that the tweet is associated (but not necessarily originating from) a Place. And as we can see on the head of the table, the place is not always set.\n",
    "\n",
    "We decided to use the longitude/latitude columns to represent the position of a tweet and if they are null we will use the placeLatitude and placeLongitude. If both are null we will have to drop the entry as a tweet without position is not usefull for event detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         int64\n",
       "userId                     int64\n",
       "createdAt         datetime64[ns]\n",
       "text                      object\n",
       "longitude                float64\n",
       "latitude                 float64\n",
       "placeLatitude            float64\n",
       "placeLongitude           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(replace_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the columns placeLongitude and placeLatitude as they don't give us anymore informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.drop(['placeLatitude', 'placeLongitude'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we drop the NaN values in Longitude and Latitude columns as we need a position to detect event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_before = len(tweets.index)\n",
    "tweets = tweets.dropna(subset=['longitude', 'latitude'])\n",
    "len_after = len(tweets.index)\n",
    "print(\"Number of tweets before dropping the one without position : \", len_before)\n",
    "print(\"Number of tweets before dropping the one without position : \", len_after)\n",
    "print(\"Percentage of tweets lost : \", ((len_before - len_after)/len_before)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we also base our event detection on the text field we don't want to have nan value in it. So we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_before = len(tweets.index)\n",
    "tweets = tweets.dropna(subset=['text'])\n",
    "len_after = len(tweets.index)\n",
    "print(\"Number of tweets before dropping the one without text : \", len_before)\n",
    "print(\"Number of tweets before dropping the one without text : \", len_after)\n",
    "print(\"Percentage of tweets lost : \", ((len_before - len_after)/len_before)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to detect the event from the tweets we now extracted. To do so we had to do some assumptions. We decided to work with the text to find event. \n",
    "- Hashtags can be a good estimators of what event were on at the time of the tweets.\n",
    "- Events can take place on several days but we will decide to take tweets day per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we decided to detect an event by its day of occurence we create a new column that gives us the information of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['day'] = pd.DatetimeIndex(tweets['createdAt']).normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to remove the stopwords from the tweets' text to keep only words that can describe an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words += stopwords.words('french')\n",
    "stop_words += stopwords.words('german')\n",
    "stop_words += stopwords.words('italian')\n",
    "stop_words += string.punctuation\n",
    "stop_words += ['—','/via','i\\'m', '^_^', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "':c', ':{', '>:\\\\', ';(', ':-)', ':)', ';)','[=o)]', ';-)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "'=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "'<3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(row):\n",
    "    text = row['text']\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@ \\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    text = text.split()\n",
    "    text  = [word for word in text if word.lower() not in stop_words]\n",
    "    row['text'] = text\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the stopwords but we also remove the URLs and the @ mentions as they are not useful to detect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(text_process, axis=1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text is pretty much clean we want to get the hashtags from the tweets because they are really helpful to detect the events. So we create a column with the hashtags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets['hashtags'] = tweets['text'].apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the \"text\" fields we don't want to have '#' anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(remove_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENT  DETECTION\n",
    "\n",
    "Now that our dataframe is clean we want to find the event hidden in all those tweets. Here are our assumptions to find an event :\n",
    "- An event is described by a list of words (hashtags are also used with greater importance than other words)\n",
    "- An event takes place at a certain place described by longitude/latitude (we took around 10km)\n",
    "- An event occurs during a certain  time (we decided to find them day by day)\n",
    "- An event has a minimum number of people talking about it (we took 3) and a minimum of tweets (we took 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find where the tweets are sent from to imagine where the clusters will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvs = ds.Canvas(plot_width=800, plot_height=800)\n",
    "agg = cvs.points(tweets, 'longitude', 'latitude')\n",
    "tf.shade(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see the map of Switzerland and most users in Switzerland can be found in Geneva, Lausanne and Zürich. There are other regions with some activity too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to detect event day per day we will have to go through all the day in our data and for each of them detect the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days = np.unique(tweets['day'].values)\n",
    "print(len(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One day detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day = tweets[tweets['day']==days[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meters = 10000\n",
    "eps = meters / 100000 # meters to degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = day.as_matrix(columns=['latitude', 'longitude'])\n",
    "db = DBSCAN(eps=eps, min_samples=5).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = day.assign(cluster=db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analysis to detect events\n",
    "\n",
    "We go through the clusters and find out if the corresponding tweets are linked by an event or not. To do so we first of all create a dataframe that will contain the events. (Event name, Event keywords, Event hashtags, Event longitude/latitude, Number of tweets, Number of people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.DataFrame(columns=['name', 'date', 'keywords', 'hashtags', 'longitude', 'latitude', '# of tweets', '# of people'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to populate our dataframe with real event. So we go through all the clusters and find out the hashtags, keywords and corresponding event based on these two infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in days:\n",
    "    day_df = tweets[tweets['day']==day]\n",
    "    \n",
    "    X = day_df.as_matrix(columns=['latitude', 'longitude'])\n",
    "    db = DBSCAN(eps=eps, min_samples=5).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    day_df = day_df.assign(cluster=db.labels_)\n",
    "    \n",
    "    #Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    for cluster in range(0,n_clusters_-1):\n",
    "        ntweets = len(day_df[day_df['cluster']==cluster])\n",
    "        npeople = len(np.unique(day_df[day_df['cluster']==cluster].userId))\n",
    "        #print(\"Cluster n°\" + str(cluster) + \"( \" + str(ntweets) + \" tweets\" + \" )\")\n",
    "        d_hashtags = dictionnary_from_hashtags(day_df, cluster)\n",
    "        d_keywords = dictionnary_from_keywords(day_df, cluster)\n",
    "\n",
    "        d_hashtags_detection = useful_(d_hashtags, 1/3, ntweets)\n",
    "        d_keywords_detection = useful_(d_keywords, 1/4, ntweets)\n",
    "\n",
    "        if(not any(d_hashtags_detection)):\n",
    "            if(any(d_keywords_detection)):\n",
    "                pass\n",
    "                # TODO only keywords\n",
    "        else:\n",
    "            position = find_position(day_df, cluster, ntweets)\n",
    "            i = len(df_event)\n",
    "            name = \"\"\n",
    "            for elem in d_hashtags_detection.keys(): \n",
    "                name += elem + \" \"\n",
    "            df_event.loc[i] = ([name, day, d_keywords_detection.keys(), d_hashtags_detection.keys(), position[0], position[1], ntweets, npeople])\n",
    "            print(\" ------- Event added -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid having spam event in our event dataframe. It means that the ratio tweet per user should not be too big. Here we decided to set this ratio to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event = df_event[df_event[\"# of tweets\"]/df_event[\"# of people\"] < 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to have more than 2 people per event and more than 7 tweets per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = df_event[(df_event[\"# of people\"] >= 3) & (df_event[\"# of tweets\"] >= 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT TO DO IF ONLY KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
