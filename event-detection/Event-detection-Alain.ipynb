{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter-Swisscom Project\n",
    "\n",
    "## EVENT DETECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from utils import *\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.cluster.hierarchy import ward, dendrogram,fcluster\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_event_split = ['id','userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'placeLatitude', 'placeLongitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse_dates = ['createdAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../twitter-swisscom/twex_event_corrected.tsv', sep=\"\\t\", encoding='utf-8', escapechar='\\\\', names=col_event_split, parse_dates=parse_dates, na_values='N', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use the 'placeId' and the 'inReplayTo' informations to do our detection, so we drop them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.drop(['placeId', 'inReplyTo'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different informations on the longitude and latitude are given, the columns 'longitude'/'latitude' are the position of the Tweet as reported by the user or client application. The 'placeLongitude'/'placeLatitude' indicates that the tweet is associated to a place. And as we can see on the head of the table, the place is not always set.\n",
    "\n",
    "We decided to use the longitude/latitude columns to represent the position of a tweet and if they are null we will use the placeLatitude and placeLongitude. If both are null we will have to drop the entry as a tweet without position is not usefull for event detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(replace_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the columns 'placeLongitude' and 'placeLatitude' as they don't give us anymore informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.drop(['placeLatitude', 'placeLongitude'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we drop the NaN values in Longitude and Latitude columns as we need a position to detect event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_before = len(tweets.index)\n",
    "tweets = tweets.dropna(subset=['longitude', 'latitude'])\n",
    "len_after = len(tweets.index)\n",
    "print(\"Number of tweets before dropping the one without position : \", len_before)\n",
    "print(\"Number of tweets before dropping the one without position : \", len_after)\n",
    "print(\"Percentage of tweets lost : \", ((len_before - len_after)/len_before)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we also base our event detection on the text field we don't want to have nan value in it. So we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_before = len(tweets.index)\n",
    "tweets = tweets.dropna(subset=['text'])\n",
    "len_after = len(tweets.index)\n",
    "print(\"Number of tweets before dropping the one without text : \", len_before)\n",
    "print(\"Number of tweets before dropping the one without text : \", len_after)\n",
    "print(\"Percentage of tweets lost : \", ((len_before - len_after)/len_before)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to detect an event by its day of occurence we create a new column that gives us the information of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['day'] = pd.DatetimeIndex(tweets['createdAt']).normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to remove the stopwords from the tweets' text to keep only words that can describe an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words += stopwords.words('french')\n",
    "stop_words += stopwords.words('german')\n",
    "stop_words += stopwords.words('italian')\n",
    "stop_words += string.punctuation\n",
    "stop_words += ['â€”','/via','via', 'follow', 'please', 'i\\'m', '^_^', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "':c', ':{', '>:\\\\', ';(', ':-)', ':)', ';)','[=o)]', ';-)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "'=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "'<3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(row):\n",
    "    text = row['text']\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@ \\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    text = text.split()\n",
    "    text  = [word for word in text if word.lower() not in stop_words]\n",
    "    row['text'] = text\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the stopwords but we also remove the URLs and the @ mentions as they are not useful to detect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(text_process, axis=1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text is pretty much clean we want to get the hashtags from the tweets because they are really helpful to detect the events. So we create a column with the hashtags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets['hashtags'] = tweets['text'].apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the \"text\" fields we don't want to have '#' anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(remove_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the current dataframe so we don't need to re-run the preprocessing everytime as it takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.to_csv(\"../twitter-swisscom/twex_event_tweets_processed.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENT  DETECTION\n",
    "\n",
    "Now that our dataframe is clean we want to find the event hidden in all those tweets. Here are our assumptions to find an event :\n",
    "- An event is described by a hashtags it contains\n",
    "- An event takes place at a certain place described by longitude/latitude (we took a radius of 10km)\n",
    "- An event occurs during a certain  time (we decided to find them day by day)\n",
    "- An event has a minimum number of people talking about it (we took 3) and a minimum of tweets (we took 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the csv file containing our tweets preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../twitter-swisscom/twex_event_tweets_processed.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to detect event day per day we will have to go through all the day in our data and for each of them detect the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days = np.unique(tweets['day'].values)\n",
    "print(len(days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection on one day\n",
    "\n",
    "Before applying the detection on the whole data, we want to do the detection on a specific day so that the pipeline is easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "index = randint(0,len(days))\n",
    "\n",
    "day = tweets[tweets['day']==days[index]]\n",
    "day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the radius where we want to find the related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meters = 10000\n",
    "eps = meters / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to detect clusters of tweets that are close to each others. And we use this cluster assignment to detect events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = day.as_matrix(columns=['latitude', 'longitude'])\n",
    "db = DBSCAN(eps=eps, min_samples=5).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = day.assign(cluster=db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the clusters position for this given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataframe containing the events\n",
    "\n",
    "We go through the clusters and find out if the corresponding tweets are linked by an event or not. To do so we first of all create a dataframe that will contain the events. (Name, date, keywords, hashtags, longitude, latitude, #of tweets, #of people, tweetids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.DataFrame(columns=['name', 'date', 'keywords', 'hashtags', 'longitude', 'latitude', '# of tweets', '# of people', 'tweetids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to populate our dataframe with real event. So we go through all the clusters and find out the hashtags, keywords and corresponding event based on these two infos. We use the same pipeline we used above to detect the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in days:\n",
    "    day_df = tweets[tweets['day']==day]\n",
    "    \n",
    "    X = day_df.as_matrix(columns=['latitude', 'longitude'])\n",
    "    db = DBSCAN(eps=eps, min_samples=5).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    day_df = day_df.assign(cluster=db.labels_)\n",
    "    \n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    for cluster in range(0,n_clusters_-1):\n",
    "        ntweets = len(day_df[day_df['cluster']==cluster])\n",
    "        npeople = len(np.unique(day_df[day_df['cluster']==cluster].userId))\n",
    "        ids = list(day_df[day_df['cluster']==cluster].id.values)\n",
    "        d_hashtags = dictionnary_from_hashtags(day_df, cluster)\n",
    "        d_keywords = dictionnary_from_keywords(day_df, cluster)\n",
    "\n",
    "        d_hashtags_detection = useful_(d_hashtags, 1/3, ntweets)\n",
    "        d_keywords_detection = useful_(d_keywords, 1/4, ntweets)\n",
    "\n",
    "        if(any(d_hashtags_detection)):\n",
    "            position = find_position(day_df, cluster, ntweets)\n",
    "            i = len(df_event)\n",
    "            name = \"\"\n",
    "            for elem in d_hashtags_detection.keys(): \n",
    "                name += elem + \" \"\n",
    "            df_event.loc[i] = ([name[:-1], day, list(d_keywords_detection.keys()), list(d_hashtags_detection.keys()), position[0], position[1], ntweets, npeople, ids])\n",
    "            print(\" ------- Event added -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_event_no_filter = len(df_event)\n",
    "print(\"Number of event without filtering the spam neither the event not in the scope : \", len_event_no_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid having spam event in our event dataframe. Here are the 3 assumptions we did to avoid the spam events :\n",
    "- (number of tweets) / (number of people) has to be less than 6\n",
    "- number of people involved needs to be greater or equal to 3.\n",
    "- number of tweets per event needs to be greater or equal to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event = df_event[df_event[\"# of tweets\"]/df_event[\"# of people\"] < 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = df_event[(df_event[\"# of people\"] >= 3) & (df_event[\"# of tweets\"] >= 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_event_no_spam = len(df_event)\n",
    "print(\"Number of event after filtering the spam : \", len_event_no_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset should be containing tweets between 2010 and 2016 but we still want to make sure we don't have outliers so we filter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event['date'] = pd.DatetimeIndex(df_event['date']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = df_event[(df_event.date >= \"2010-01-01\") & (df_event.date <= \"2016-12-31\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our dataframe to find the countries linked with each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/twex_event_no_country.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pipeline we used to find the country given the longitude/latitude of an event :\n",
    "- Install OSGeo4W : https://trac.osgeo.org/osgeo4w/wiki\n",
    "- Open the OSGeo4W shell\n",
    "- Go to the directory where our script is (/country-detection)\n",
    "- install pandas : pip install pandas\n",
    "- run the script : python find_country.py\n",
    "\n",
    "\n",
    "\n",
    "(in our script we use these two tools : https://github.com/che0/countries)\n",
    "\n",
    "We then reload the dataframe with the countries and remove the one that are not in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(\"../twitter-swisscom/twex_event_country.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_total = len(df_event)\n",
    "df_event = df_event[df_event.country == 'CH']\n",
    "len_swiss = len(df_event)\n",
    "print(\"Total number of events : \", len_total)\n",
    "print(\"Number of events in Switzerland : \", len_swiss)\n",
    "print(\"Loss percentage : \", (1-(len_swiss/len_total))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_event_final = len(df_event)\n",
    "print(\"Number of event after filtering the event not in the scope (temporal and spatial) : \", len_event_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our dataframe to a csv as all the detection is done and we want to use it for our visualization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/twex_event_final_events.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After visualization correction\n",
    "\n",
    "We reload our csv and make some tweaks to make our detection more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(\"../twitter-swisscom/twex_event_final_events.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vizualizing our data on several years, we see that some events seems to not be spatial but only like trending hashtags, we can detect them by finding events that take place at the same time but in many different places. We don't want to show them on the map as there position is not really representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event['trending'] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trending_event = df_event.groupby(['name','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namelist = []\n",
    "def getnames(group):\n",
    "    name = group.name\n",
    "    if len(group['longitude'])>1:\n",
    "        namelist.append(name[0])\n",
    "\n",
    "trending_event.apply(getnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_event['trending'] = df_event['name'].isin(namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df_event[df_event['trending']==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found out that some events were duplicated because they are were taking place on different following days. We don't want to make two different bubbles on our vizualization for the same event, so we will group them. We do it in our vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final event dataframe is saved and used in the viz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/event_detected_final.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
