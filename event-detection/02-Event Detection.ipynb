{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis - Fall 2016\n",
    "## Twitter-Swisscom Project\n",
    "\n",
    "### Event Detection\n",
    "\n",
    "This notebook contains all the operation we made on the data to detect events in Switzerland as precisely as possible.\n",
    "Here is a summary of what we did :\n",
    "\n",
    "1 - [Load the data](#load_data)\n",
    "\n",
    "2 - [Detection for one day columns](#one_day_detection)\n",
    "\n",
    "&nbsp;&nbsp; 2.1 - [Use of DBSCAN for clustering](#DBSCAN)\n",
    "\n",
    "3 - [Run detection on the whole data](#whole_data)\n",
    "\n",
    "4 - [Scope of the events detected](#event_scope)\n",
    "\n",
    "5 - [Remove stopword from text](#remove_stopwords)\n",
    "\n",
    "6 - [Get the hashtags from the text](#get_hashtags)\n",
    "\n",
    "7 - [Save the tweets into csv for event Detection](#save_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Now that our dataframe is clean we want to find the event hidden in all those tweets. Here are our assumptions to find an event :\n",
    "\n",
    "- An event takes place at a certain place described by longitude/latitude. (we take a radius of 3km)\n",
    "- An event is described by hashtags it contains but this hashtags needs to be in at least 1/10 of tweets  a cluster contains.\n",
    "- An event occurs during a certain  time. (we decide to find them day by day)\n",
    "\n",
    "Avoid spam events :\n",
    "- An event has a minimum number of people talking about it (we take 20) and a minimum of tweets. (we take 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - <a id='load_data'>Load the data</a> \n",
    "We load the csv file containing our tweets preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../twitter-swisscom/event/tweets_processed.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - <a id='one_day_detection'>Detection for one day<a>\n",
    "\n",
    "Before applying the detection on the whole data, we want to do the detection on a specific day so that the pipeline is easy to understand.\n",
    "\n",
    "We need to know how many days we have in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = np.unique(tweets['day'].values)\n",
    "ndays = len(days)\n",
    "print(ndays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take one of the day to show the detection on one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'days' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b54ceae59bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'days' is not defined"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "index = randint(0,ndays)\n",
    "\n",
    "day = tweets[tweets['day']==days[index]]\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - <a id='DBSCAN'>Use of DBSCAN for clustering<a>\n",
    "\n",
    "We use [DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) to detect clusters of tweets that are close to each others. And we use this cluster assignment to detect events. Here are the parameters value we use to obtain our final result with explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPS = 3/6371 #radius of clusters (3km)\n",
    "MIN = 5 #The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "ALGORITHM = 'ball_tree' #The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors\n",
    "METRIC = 'haversine' #The metric to use when calculating distance between instances, haversine is for longitude/latitude in radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.radians(day.as_matrix(columns=['latitude', 'longitude']))\n",
    "db = DBSCAN(eps=EPS, min_samples=MIN, algorithm=ALGORITHM metric=METRIC).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "day = day.assign(cluster=db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of clusters created on this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot the clusters detected in function of the longitude/latitude of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - <a id='whole_data'>Run detection on the whole data<a>\n",
    "\n",
    "We go through the clusters and find out if the corresponding tweets are linked by an event or not. To do so we first of all create a dataframe that will contain the events. (Name, date, keywords, hashtags, longitude, latitude, #of tweets, #of people, tweetids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.DataFrame(columns=['name', 'date', 'keywords', 'hashtags', 'longitude', 'latitude', '# of tweets', '# of people', 'tweetids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to populate our dataframe with real event. So we go through all the clusters and find out the hashtags, keywords and corresponding event based on these two infos. We use the same pipeline we used above to detect the clusters.\n",
    "\n",
    "To detect hashtags and keywords we use the assumptions we did at the start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_=0\n",
    "print('Number of days to go through : ', ndays)\n",
    "for day in days:\n",
    "    if(iter_%100==0):\n",
    "        print(str(iter_) + '/' + str(ndays))\n",
    "    iter_+=1\n",
    "    day_df = tweets[tweets['day']==day]\n",
    "    \n",
    "    X = np.radians(day_df.as_matrix(columns=['latitude', 'longitude']))\n",
    "    db = DBSCAN(eps=EPS, min_samples=MIN, algorithm=ALGORITHM, metric=METRIC).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    day_df = day_df.assign(cluster=db.labels_)\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    for cluster in range(0,n_clusters_-1):\n",
    "        ntweets = len(day_df[day_df['cluster']==cluster])\n",
    "        npeople = len(np.unique(day_df[day_df['cluster']==cluster].userId))\n",
    "        # Here we test if the number of people and the number of tweets are high enough to be detected as events\n",
    "        if(ntweets>=50 & npeople>=20): \n",
    "            ids = list(day_df[day_df['cluster']==cluster].id.values)\n",
    "            d_hashtags = dictionnary_from_hashtags(day_df, cluster)\n",
    "            d_keywords = dictionnary_from_keywords(day_df, cluster)\n",
    "\n",
    "            d_hashtags_detection = useful_(d_hashtags, 1/10, ntweets)\n",
    "            d_keywords_detection = useful_(d_keywords, 1/10, ntweets)\n",
    "\n",
    "            if(any(d_hashtags_detection)):\n",
    "                position = find_position(day_df, cluster, ntweets)\n",
    "                i = len(df_event)\n",
    "                name = \"\"\n",
    "                for elem in d_hashtags_detection.keys(): \n",
    "                    name += elem + \" \"\n",
    "                df_event.loc[i] = ([name[:-1], day, list(d_keywords_detection.keys()), list(d_hashtags_detection.keys()), position[0], position[1], ntweets, npeople, ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the number of events we detected and the head of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_event = len(df_event)\n",
    "print(\"Number of event without filtering the spam neither the event not in the scope : \", len_event)\n",
    "df_event.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - <a id='event_scope'>Scope of the event detected</a>\n",
    "\n",
    "Our dataset should be containing tweets between 2010 and 2016 but we still want to make sure we don't have outliers so we filter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event['date'] = pd.DatetimeIndex(df_event['date']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = df_event[(df_event.date >= \"2010-01-01\") & (df_event.date <= \"2016-12-31\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our dataframe to find the countries linked with each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/event/event_no_country.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - <a id='country_detection'>Event's country detection</a>\n",
    "\n",
    "Here is the pipeline we used to find the country given the longitude/latitude of an event :\n",
    "- Install OSGeo4W : https://trac.osgeo.org/osgeo4w/wiki\n",
    "- Open the OSGeo4W shell\n",
    "- Go to the directory where our script is (/country-detection)\n",
    "- install pandas : pip install pandas\n",
    "- run the script : python find_country.py\n",
    "\n",
    "\n",
    "(in our script we use these two tools : https://github.com/che0/countries)\n",
    "\n",
    "We then reload the dataframe with the countries and remove the one that are not in Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(\"../twitter-swisscom/event/event_country.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep only events that are in Switzerland to show them on our viz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_total = len(df_event)\n",
    "df_event = df_event[df_event.country == 'CH']\n",
    "len_swiss = len(df_event)\n",
    "print(\"Total number of events : \", len_total)\n",
    "print(\"Number of events in Switzerland : \", len_swiss)\n",
    "print(\"Loss percentage : \", (1-(len_swiss/len_total))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/event/event_final_events.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - <a id='trending'>Trending events detection</a>\n",
    "\n",
    "After vizualizing our data on several years, we see that some events seems to not be spatial but only like trending hashtags, we can detect them by finding events that take place at the same time but in many different places. We don't want to show them on the map as there position is not really representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(\"../twitter-swisscom/event_final_events.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column with the 'Trending' value equals to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event['trending'] = \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect same events that are on the same day at different locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trending_event = df_event.groupby(['name','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namelist = []\n",
    "def getnames(group):\n",
    "    name = group.name\n",
    "    if len(group['longitude'])>1:\n",
    "        namelist.append(name[0])\n",
    "\n",
    "trending_event.apply(getnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set their 'Trending' value to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event['trending'] = df_event['name'].isin(namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of trending events detected : ', len(df_event[df_event['trending']==True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - <a id='save_data'>Save the events</a>\n",
    "Our final event dataframe is saved and used in the viz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_event.to_csv(\"../twitter-swisscom/event/events_detected.csv\", sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
