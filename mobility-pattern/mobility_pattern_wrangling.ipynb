{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis - Fall 2016\n",
    "## Twitter-Swisscom Project\n",
    "\n",
    "### Mobility Pattern: Data Wrangling\n",
    "\n",
    "\n",
    "1 - [Load the data](#load_data)\n",
    "\n",
    "2 - [Cleaning](#cleaning)\n",
    "\n",
    "3 - [Exploratory and Analysis](#exploratory_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from mobility_helper import *\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from geopy.geocoders import Nominatim,Bing\n",
    "import datetime as dt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - <a id='load_data'>Load the data</a> \n",
    "\n",
    "Contains subset of the original schema. Already removed useless columns for mobility pattern at preprocessing:\n",
    "1. id\n",
    "2. userID\n",
    "3. createdAt\n",
    "4. longitude\n",
    "5. latitude\n",
    "6. placeID\n",
    "7. placeLatitude\n",
    "8. placeLongitude\n",
    "9. sourceName\n",
    "10. userLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '../twitter-swisscom/twex_mobility_corrected.tsv'\n",
    "columns = ['ID', 'userID', 'createdAt', 'longitude', 'latitude', 'placeID','placeLatitude','placeLongitude','sourceName', 'userLocation']\n",
    "dtypes = {'ID': 'int', 'createdAt': 'str', 'longitude': 'float', 'latitude': 'float', 'placeID': 'str', 'placeLatitude': 'float', 'placeLongitude': 'float', 'sourceName': 'str', 'userLocation': 'str'}\n",
    "\n",
    "chunk_size = 10**6\n",
    "df = pd.read_csv(src, sep='\\t', names=columns, na_values=['\\\\N'],dtype=dtypes, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - <a id='cleaning'>Cleaning</a>\n",
    "\n",
    "We check the number of rows with null latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(df[pd.isnull(df.latitude)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of null longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(df[pd.isnull(df.longitude)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of tweets with both latitude and longitude null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_coord = np.logical_and(pd.isnull(df.latitude), pd.isnull(df.longitude))\n",
    "tw_wo_coord = df[no_coord]\n",
    "print(len(tw_wo_coord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same numbers. Thus one is never null without the second also null.\n",
    "\n",
    "We now check if the place coordinates exists in case of null original coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "has_place_coord = tw_wo_coord[~np.logical_and(pd.isnull(tw_wo_coord.placeLatitude), pd.isnull(tw_wo_coord.placeLongitude))]\n",
    "len(has_place_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they always exist. This imply that we can replace the original coordinates by the place coordinates in case of nullity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['latitude'] = df.apply(replaceLatitude, axis=1)\n",
    "df['longitude'] = df.apply(replaceLongitude, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the number of tweets with no coordinates now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_coord = np.logical_and(pd.isnull(df.latitude), pd.isnull(df.longitude))\n",
    "len(df[no_coord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have NaN coordinates. We can now safely drop the place coordinates columns since they are no longer of use to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['placeID', 'placeLatitude', 'placeLongitude'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at timestamps. We start by checking the nullity of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(df[pd.isnull(df.createdAt)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no tweets without timestamp.\n",
    "\n",
    "We now want to have tweets in the area of Switzerland with pieces of the neighboring countries. We thus filter by latitude and longitude. we define the following bounds:\n",
    "- 45 <= latitude <= 48\n",
    "- 4 <= longitude <= 13\n",
    "We remove the potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "lat_inbound = ((45 <= df.latitude) & (df.latitude <= 48))\n",
    "long_inbound = ((4 <= df.longitude) & (df.longitude <= 13))\n",
    "inbound = np.logical_and(lat_inbound, long_inbound)\n",
    "print(len(df[~inbound]))\n",
    "df = df[inbound]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to remove the sourceName field. We wanted to see if we could emphasize on the source of the sending: Desktop or mobile. Since the majority of sources are coming from mobile apps it is of no use to us. We remove the corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['sourceName'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - <a id='exploratory_analysis'>Data exploration and Analysis</a> \n",
    "\n",
    "We first want to look at the distribution of our coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_cities = {\n",
    "    'Zurich': [47.36667, 8.55000],\n",
    "    'Geneva':[46.2,6.1667],\n",
    "    'Basel': [47.5667,7.6],\n",
    "    'Bern' : [46.9167,7.4667],\n",
    "    'Lausanne': [46.5333,6.6667],\n",
    "    'Sion': [46.2333,7.35],\n",
    "    'Varese': [45.8176,8.8264],\n",
    "    'Mulhouse': [47.75, 7.3333],\n",
    "}\n",
    "g = sns.jointplot(x=df.longitude, y=df.latitude, kind=\"hex\", color=\"k\");\n",
    "for city, coord in main_cities.items():\n",
    "    g.ax_joint.scatter(coord[1], coord[0], marker='o', c='r', s=5)\n",
    "    g.ax_joint.annotate(city, xy=(coord[1], coord[0]), xytext=(coord[1], coord[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our coordinate are mostly concentrated in the big cities of Switzerland as well as some  bordering cities.\n",
    "\n",
    "After looking at the user location we see that this information is messy and frequently useless to us. It is too difficult to detect and use this field when it adds good information. We remove the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['userLocation'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group our tweets by users and check the distribution of tweets per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_users = pd.DataFrame(df.groupby(['userID']).size())\n",
    "count_users.columns = ['count']\n",
    "count_users = count_users.sort_values(by=['count'], ascending=True)\n",
    "count_users = count_users.reset_index()\n",
    "\n",
    "count_users.plot(y='count', use_index=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe than 5% of our users wrote more than 90% of our tweets. This means that our number of users will drastically be inferior to the number of tweets we have. Also, this also mean that we have a lot of users that wrote very few tweets. Since we want to do statistics about users locations and movement we should remove those users.\n",
    "\n",
    "We remove users who wrote less than 100 of tweets. We will loose a high number of users but the ones we keep will have enough data to obtain statistics of minimum quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "before = len(count_users)\n",
    "count_users = count_users[count_users['count'] >= threshold]\n",
    "after = len(count_users)\n",
    "print('Percentage of user loss: ',(before-after)*100/before)\n",
    "\n",
    "#list of ids to keep\n",
    "#DataFrame 'tweets' is the filtered version of our first 'df'\n",
    "ids_to_keep = list(count_users.userID)\n",
    "tweets = df[df.userID.isin(ids_to_keep)]\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check our timestamps. We want to know if we have tweets without timestamps and we want to convert those into datetime objects for future uses.\n",
    "\n",
    "Check the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['createdAt'] = pd.DatetimeIndex(tweets['createdAt'])\n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets.reset_index(drop=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cleaned our dataset and save the obtained dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_csv('./data/tweets_processed.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
