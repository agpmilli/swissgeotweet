{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis - Fall 2016\n",
    "## Twitter-Swisscom Project\n",
    "\n",
    "### Mobility Pattern\n",
    "\n",
    "\n",
    "1 - [Coordinate rounding](#rounding)\n",
    "\n",
    "2 - [Home/Work Detection](#home_work)\n",
    "\n",
    "3 - [Route Computing](#route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from mobility_helper import *\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from geopy.geocoders import Nominatim,Bing\n",
    "import datetime as dt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load our cleaned and preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./data/tweets_preprocessed.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - <a id='rounding'>Coordinate rounding</a>\n",
    "\n",
    "The location coordinate that we have are too precise. It will be impossible to group by coordinate in order to detect key places like home and work. We thus decide to round the latitude and longitude of each tweets. We do this by lowering the coordinate precision (# of decimals).\n",
    "\n",
    "C.f. https://en.wikipedia.org/wiki/Decimal_degrees\n",
    "\n",
    "- precision to 3 decimals: equateur: radius of 80m \n",
    "- precision to 2 decimals: equateur: raidus of 780m\n",
    "\n",
    "By keeping 3 decimals, we observed that a lot of places were not grouped with others. Disabling us to infer on key places. We thus applied a 2-decimals precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['areaLat'] = tweets.apply(lambda r: float('%.2f' % r.latitude), axis=1)\n",
    "tweets['areaLong'] = tweets.apply(lambda r: float('%.2f' % r.longitude), axis=1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now infer on tweets sent during work hours: 8:00-18:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['createdAt'] = pd.to_datetime(tweets['createdAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['@workHour'] = (tweets['createdAt'].dt.hour <= 18) & (tweets['createdAt'].dt.hour >= 8)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - <a id='home_work'> Home/Work place Detection </a>\n",
    "\n",
    "We want to define a key place if at least 10 tweets were sent from there, at least 2 tweets have a 24h offset and it is sent in the predefined correspondinf work or home hours.\n",
    "\n",
    "We thus defined those two functions that will help us select the given locations per users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def place_24hdiff(name, most_freq, place):\n",
    "    if place == \"work\":\n",
    "        tw = at_work[(at_work.userID == name) & (at_work.areaLong == most_freq[1]) & (at_work.areaLat == most_freq[0])]\n",
    "    else:\n",
    "        tw = nat_work[(nat_work.userID == name) & (nat_work.areaLong == most_freq[1]) & (nat_work.areaLat == most_freq[0])]\n",
    "    #return true if day distance >=1        \n",
    "    for i1, row1 in tw.iterrows():\n",
    "        for i2, row2 in tw.iterrows():\n",
    "            d = row1.createdAt - row2.createdAt\n",
    "            if abs(d.days) >= 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def most_freq_coord(group, threshold, place):\n",
    "    lat_long = list(zip(group.areaLat, group.areaLong))\n",
    "    c = Counter(lat_long)\n",
    "    most_freq = list(c)[0]\n",
    "    n = c[most_freq]\n",
    "    while not place_24hdiff(group.name, most_freq, place):\n",
    "        c.pop(most_freq, 0)\n",
    "        if len(c) == 0:\n",
    "            #no places with valid diff: return last one with frequence 0 which will be deleted with threshold\n",
    "            return pd.Series({'freqLat': most_freq[0], 'freqLong': most_freq[1], 'frequence': 0})\n",
    "        else:\n",
    "            most_freq = list(c)[0]\n",
    "            n = c[most_freq]\n",
    "    #out of while loop: means we have a valid place (1 day diff)\n",
    "    return pd.Series({'freqLat': most_freq[0], 'freqLong': most_freq[1], 'frequence': n})\n",
    "\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work place detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "at_work = tweets[tweets['@workHour']]\n",
    "at_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_freq = at_work.groupby(['userID']).apply(lambda g : most_freq_coord(g,threshold, \"work\"))\n",
    "print(len(work_freq))\n",
    "work_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the threshold. And save the index (userID) that we should remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(work_freq[work_freq.frequence < threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_to_remove = work_freq[work_freq.frequence < threshold].index.values.tolist()\n",
    "len(work_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home place detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nat_work = tweets[~tweets['@workHour']]\n",
    "nat_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_freq = nat_work.groupby(['userID']).apply(lambda g: most_freq_coord(g, threshold, \"home\"))\n",
    "print(len(home_freq))\n",
    "home_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the threshold and save the index (userID) that we should remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(home_freq[home_freq.frequence < threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_to_remove = home_freq[home_freq.frequence < threshold].index.values.tolist()\n",
    "len(home_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merge\n",
    "\n",
    "We merge the 2 dataframes: work_freq, home_freq.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.merge(work_freq, home_freq,how='inner', left_index=True, right_index=True)\n",
    "columns = ['workLat', 'workLong', 'workTweets', 'homeLat', 'homeLong', 'homeTweets']\n",
    "users.columns = columns\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users['userID'] = users.index.values\n",
    "users = users.reset_index(drop=True)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove users that did not validate conditions on work/home tweet frequence threshold before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_remove = home_to_remove + work_to_remove\n",
    "print(len(users))\n",
    "users = users[~users.userID.isin(to_remove)]\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - <a id='route'> Route Computing </a>\n",
    "\n",
    "We want to compute the canton and country of the home and work locations that we detected. Then we want to compute the distance from the work place to the home place while finally computing the time it takes by car ride.\n",
    "\n",
    "### Administrative location computation\n",
    "\n",
    "Location query and storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "countries = []\n",
    "cache = {}\n",
    "for i, row in users.iterrows():\n",
    "    w = 0\n",
    "    h = 0\n",
    "    if cache.get((row.homeLat, row.homeLong)):\n",
    "        home = cache[(row.homeLat, row.homeLong)]\n",
    "        h = home.address.split(', ')[-1]\n",
    "    else:\n",
    "        home = geolocator.reverse(str(row.homeLat)+\", \"+str(row.homeLong))#.address.split(', ')[-1]\n",
    "        cache[(row.homeLat, row.homeLong)] = home\n",
    "        h = home.address.split(', ')[-1]\n",
    "        time.sleep(0.5)\n",
    "    if cache.get((row.workLat, row.workLong)):\n",
    "        work = cache[(row.workLat, row.workLong)]\n",
    "        w = work.address.split(', ')[-1]\n",
    "    else:\n",
    "        work = geolocator.reverse(str(row.workLat)+\", \"+str(row.workLong))#.address.split(', ')[-1]\n",
    "        cache[(row.workLat, row.workLong)] = work\n",
    "        w = work.address.split(', ')[-1]\n",
    "        time.sleep(0.5)\n",
    "    if w == \"Svizra\":\n",
    "        w = \"Suisse\"\n",
    "    if h == \"Svizra\":\n",
    "        h = \"Suisse\"\n",
    "    countries.append((h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries: We add countries to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = pd.DataFrame(countries, columns=['homeCountry', 'workCountry'])\n",
    "users = pd.concat([users, c], axis=1)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantons: we add the cantons to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#detect Cantons\n",
    "def get_cantons(row):\n",
    "    if row.homeCountry == \"Suisse\":\n",
    "        homecant = cache[(row.homeLat, row.homeLong)].address.split(', ')[-6]\n",
    "    else:\n",
    "        homecant = \"-\"\n",
    "    if row.workCountry == \"Suisse\":\n",
    "        workcant = cache[(row.workLat, row.workLong)].address.split(', ')[-6]\n",
    "    else:\n",
    "        workcant = \"-\"\n",
    "    return (homecant, workcant)\n",
    "\n",
    "cantons = list(users.apply(get_cantons, axis=1))\n",
    "cantons = pd.DataFrame(cantons, columns=['homeCanton', 'workCanton'])\n",
    "users = pd.concat([users, cantons], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Home-Work computing\n",
    "\n",
    "We start by defining functions helping us computing distance from coordinate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def dist(row):\n",
    "    if (row.workLat == row.homeLat) and (row.workLong == row.homeLong):\n",
    "        return 0\n",
    "    else:\n",
    "        return haversine(row.workLong, row.workLat, row.homeLong, row.homeLat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now defined a new column giving the home-work distance of each users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users['distance'] = users.apply(dist, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection of closer main cities (work/home)\n",
    "\n",
    "Definition of main cities and definition of new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_cities = {\n",
    "    'Zurich': [47.36667, 8.55000],\n",
    "    'Geneva':[46.2,6.1667],\n",
    "    'Basel': [47.5667,7.6],\n",
    "    'Bern' : [46.9167,7.4667],\n",
    "    'Lausanne': [46.5333,6.6667],\n",
    "    'Luzern': [47.0833,8.2667],\n",
    "    'Sion': [46.2333,7.35],\n",
    "    'Varese': [45.8176,8.8264],\n",
    "    'Mulhouse': [47.75, 7.3333],\n",
    "    'Annecy': [45.9,6.1167],\n",
    "    'Annemasse': [46.1944, 6.2377],\n",
    "    'Pontarlier': [46.9035,6.3554],\n",
    "    'Aoste': [45.5833, 5.6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closer(row, where):\n",
    "    if where == 'work':\n",
    "        lat = row.workLat\n",
    "        long = row.workLong\n",
    "    else:\n",
    "        lat = row.homeLat\n",
    "        long = row.homeLong\n",
    "    d = 10000\n",
    "    closer = \"\"\n",
    "    for city, coord in main_cities.items():\n",
    "        new = haversine(long, lat, coord[1], coord[0])\n",
    "        if new < d:\n",
    "            d = new\n",
    "            closer = city\n",
    "    return (closer, d)\n",
    "\n",
    "\n",
    "users['closer to home'] = users.apply(lambda r: get_closer(r, \"home\"), axis=1)\n",
    "users['closer to work'] = users.apply(lambda r: get_closer(r, \"work\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route time computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import codecs\n",
    "import sys\n",
    "sys.path.append('/home/hparmantier/Applied Data Analysis/SwissGeoTweet/')\n",
    "from key import *\n",
    "#API_KEY imported from file\n",
    "\n",
    "def get_time(row):\n",
    "    if (row.homeLat == row.workLat) and (row.homeLong == row.workLong):\n",
    "        return 0\n",
    "    else:\n",
    "        src = {'lat':row.homeLat, 'long':row.homeLong}\n",
    "        dest = {'lat': row.workLat, 'long': row.workLong}\n",
    "        params = {\n",
    "            'key': API_KEY,\n",
    "            'outFormat': \"json\",\n",
    "            'inFormat': \"json\"\n",
    "        }\n",
    "        request_body = {\n",
    "            'locations': [\n",
    "                {'latLng': {'lat': src['lat'], 'lng': src['long']}},\n",
    "                {'latLng': {'lat': dest['lat'], 'lng': dest['long']}}\n",
    "                ]\n",
    "            }\n",
    "        r=requests.post('https://www.mapquestapi.com/directions/v2/route',\n",
    "                        params=params, \n",
    "                        data=json.dumps(request_body)\n",
    "                       )\n",
    "        if r.status_code != 200:\n",
    "            # We didn't get a response from Mapquest\n",
    "            return -1\n",
    "        return r.json()['route']['time']/60\n",
    "\n",
    "users['routeTime'] = users.apply(get_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = users[users.routeTime != 0]\n",
    "print(len(test))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the dataframe that we created. It will be further used to extract some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users.to_csv('./data/users_final.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
