{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swiss geo tweet - Affluence map and mobility patterns in Switzerland\n",
    "\n",
    "## Abstract\n",
    "\n",
    "The aim of this project is to:\n",
    "- build a data set of Switzerland based on various social network sources (geo-located tweets and possibly instagram posts at the moment) to compute a flow analysis into an exploitable dataset for visualization (ex: timeline map, etc.)\n",
    "- infer mobility patterns from it and try to detect from regular events (ex: people living in the Vaud canton but working in the Geneva canton) to major events (ex: Paleo Festival, Geneva auto showroom)\n",
    "\n",
    "With the visualization, people will have the ability to get an affluence overview in time (months, days, hours) and space (main axes, cantons, cities, places). From there, they can filter the map and display the locations they want to visit according if it's crowded or not at that time of the year/day.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "Our main source of data will be Twitter (and possibly Instagram posts).\n",
    "\n",
    "There's already a dataset composed of tweets in Switzerland from 2012. The [Twitter API overview](https://dev.twitter.com/overview/api) gives informations about what fields can be fetched from tweets.\n",
    "\n",
    "We don't already have a dataset containing Instagram posts in Switzerland and it is part of our project to see if we can get one. The [Instagram API endpoints](https://www.instagram.com/developer/endpoints/) gives informations about what can be fetched from Instagram posts (such as the [location](https://www.instagram.com/developer/endpoints/locations/)).\n",
    "\n",
    "## Feasibility and Risks\n",
    "\n",
    "This project will require some challenging tasks. First of all, we need to get the corresponding datas. As we don't have it for Instagram posts in Switzerland, we will need to find a way to get it for some months or years if possible. We can perform some Instagram Mining using [python-instagram]( https://github.com/facebookarchive/python-instagram).\n",
    "\n",
    "For tweets, a dataset is already collected. Hence we have to extract the relevant informations which are mainly the events' hashtags, the localizations and maybe users' ids. One difficulty is that not all tweets were produced by a device enabling geo-location. It may decrease the size of our data.\n",
    "\n",
    "In addition, once our data characteristics will be extracted from the tweets, one difficulty will be to infer the users' type of locations (workplace/home or in-between point). Then, we will need to identify events' hashtags. As they are not really structured (mispellings, lot of variants for the same event, etc.), it may be difficult to infer the correct context/informations from them. It will also be interesting to do some analysis on the selected tweets's texts in order to have an idea about an event's characteristics for instance.\n",
    "\n",
    "Once both datasets are collected, we will need to merge them and think about a representation that makes the storage size not to big and that allows us to query the data easily.\n",
    "\n",
    "One point we can notice is that the data may not be representative of all the population we're interested in (Twitter and Instagram accounts tends to be more popular and used by the new generations). \n",
    "\n",
    "## Deliverables\n",
    "\n",
    "As previously explained, the final goal of this project is to deliver an exploitable data-set (ex: JSON file) of the population movements in Switzerland and its neighbor areas through time while giving some additional informations on key population gatherings such as events and others based on tweets and instagram posts.\n",
    "\n",
    "## Possible timeplan\n",
    "\n",
    "The first draft of the timeplan for this project is : \n",
    "- 1-2 weeks : research on what have been already done regarding work on mobility \n",
    "- 2 weeks : Fetching the data from Instagram for Switzerland and for some periods of time (if possible the same period as we have for the tweets : 2012-2016)\n",
    "- 4 weeks : Interpreting the given datasets of tweets given.\n",
    "- 1 week : Filtering the informations needed in these two datasets.\n",
    "- 2-3 weeks : Thinking about a memory representation that could easily fit our final vizualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import re\n",
    "import pycountry\n",
    "from os import path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shema of the data\n",
    "\n",
    "1    id    bigint(20)        UNSIGNED    No    None\n",
    "\n",
    "2    userId    bigint(20)        UNSIGNED    No    None\n",
    "\n",
    "3    createdAt    timestamp            No    0000-00-00 00:00:00\n",
    "\n",
    "4    text    text    utf8_unicode_ci        No    None\n",
    "\n",
    "5    longitude    float            Yes    NULL\n",
    "\n",
    "6    latitude    float            Yes    NULL\n",
    "\n",
    "7    placeId    varchar(25)    utf8_general_ci        Yes    NULL\n",
    "\n",
    "8    inReplyTo    bigint(20)        UNSIGNED    Yes    NULL\n",
    "\n",
    "9    source    int(10)        UNSIGNED    No    None\n",
    "\n",
    "10    truncated    bit(1)            No    None\n",
    "\n",
    "11    placeLatitude    float            Yes    NULL\n",
    "\n",
    "12    placeLongitude    float            Yes    NULL\n",
    "\n",
    "13    sourceName    varchar(255)    utf8_general_ci        Yes    NULL\n",
    "\n",
    "14    sourceUrl    varchar(255)    utf8_general_ci        Yes    NULL\n",
    "\n",
    "15    userName    varchar(200)    utf8_general_ci        Yes    NULL\n",
    "\n",
    "16    screenName    varchar(200)    utf8_general_ci        Yes    NULL\n",
    "\n",
    "\n",
    "17    followersCount    int(10)        UNSIGNED    Yes    NULL\n",
    "\n",
    "18    friendsCount    int(10)        UNSIGNED    Yes    NULL\n",
    "\n",
    "19    statusesCount    int(10)        UNSIGNED    Yes    NULL\n",
    "\n",
    "20    userLocation    varchar(200)    utf8_general_ci        Yes    NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', 'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount', 'userLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('twitter-swisscom/twex_split_1/twex_1.tsv',names=col_data, sep='\\t')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data_split = ['id', 'createdAt', 'longitude', 'latitude', 'placeId', 'placeLatitude', 'placeLongitude', 'userLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtypes = {'id' : int, 'createdAt' : str, 'longitude' : float, 'latitude' : str, 'placeId' : str, 'placeLatitude' : str, 'placeLongitude' : str , 'userLocation' : str}\n",
    "parse_dates = ['createdAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_location = pd.read_csv('twitter-swisscom/twex_preprocessed_location.tsv', sep=\"\\t\", encoding='utf-8', escapechar='\\\\', names=col_data_split, dtype=dtypes, parse_dates=parse_dates, na_values='N', header=None)\n",
    "tweets_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cannot read sample.tsv ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tweets = pd.read_csv('twitter-swisscom/sample.tsv',names=col_data, sep='\\t')\n",
    "    tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
